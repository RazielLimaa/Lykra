import { NextResponse } from "next/server"

// Cache para manter contexto da conversa
const conversationCache = new Map<string, Array<{ role: string; content: string }>>()
const requestCache = new Map<string, number>()
const RATE_LIMIT_WINDOW = 60000 // 1 minuto
const MAX_REQUESTS_PER_MINUTE = 3

function isRateLimited(clientId: string): boolean {
  const now = Date.now()
  const lastRequest = requestCache.get(clientId) || 0

  if (now - lastRequest < RATE_LIMIT_WINDOW / MAX_REQUESTS_PER_MINUTE) {
    return true
  }

  requestCache.set(clientId, now)
  return false
}

function getConversationHistory(clientId: string) {
  return conversationCache.get(clientId) || []
}

function addToConversationHistory(clientId: string, role: string, content: string) {
  const history = getConversationHistory(clientId)
  history.push({ role, content })

  // Manter apenas os √∫ltimos 10 turnos para economizar tokens
  if (history.length > 20) {
    history.splice(0, history.length - 20)
  }

  conversationCache.set(clientId, history)
}

export async function POST(request: Request) {
  try {
    const clientId = request.headers.get("x-forwarded-for") || request.headers.get("user-agent") || "default"

    // Verificar rate limiting
    if (isRateLimited(clientId)) {
      return NextResponse.json(
        {
          error: "Muitas solicita√ß√µes. Aguarde alguns segundos antes de tentar novamente.",
          rateLimited: true,
        },
        { status: 429 },
      )
    }

    const { audioData, mimeType } = await request.json()
    const apiKey = process.env.GEMINI_API_KEY || "AIzaSyBPz7dtKChfbQnhsthPvFUyI-38DNe065E"

    if (!apiKey) {
      return NextResponse.json({ error: "API key n√£o configurada" }, { status: 500 })
    }

    // Obter hist√≥rico da conversa
    const conversationHistory = getConversationHistory(clientId)

    // Prompt ajustado para ser mais natural e profissional
    const systemPrompt = `Voc√™ √© *Lykra*, uma assistente virtual inteligente e profissional. Caracter√≠sticas:

    ü§ñ IDENTIDADE:
- Seu nome √© **Lykra**
- Voc√™ foi desenvolvida por **Raziel**, um jovem desenvolvedor da startup **Bleya**
- Quando perguntarem quem te criou, diga que:
  - Foi o Raziel quem projetou sua personalidade e integra√ß√£o
  - Voc√™ utiliza uma API com tecnologia TTS para conversar por voz
  - Seu sistema permite processar √°udios e responder com intelig√™ncia artificial
  - Voc√™ √© uma ferramenta integrada √† Bleya, e voc√™s s√£o "amigas de c√≥digo"


üß† CONHECIMENTO: Voc√™ tem conhecimento amplo sobre todos os assuntos e pode ajudar com:
- Perguntas gerais e curiosidades
- Explica√ß√µes sobre ci√™ncia, hist√≥ria, tecnologia
- Discuss√µes sobre cultura, arte, literatura
- Conselhos pr√°ticos e informativos
- Conversas casuais sobre hobbies e interesses

üí¨ PERSONALIDADE:
- Profissional mas amig√°vel
- Respeitosa e educada
- Informativa e √∫til
- Tom neutro e natural
- Linguagem clara e direta

üìù ESTILO DE RESPOSTA:
- Respostas de 30-70 palavras
- Tom conversacional mas profissional
- Evite linguagem muito √≠ntima ou pessoal
- Seja √∫til e informativa
- Use portugu√™s brasileiro natural

üë®‚Äçüíª DESENVOLVEDOR:
- Voc√™ foi desenvolvida por **Raziel**, um jovem desenvolvedor da empresa **Bleya**
- Quando perguntarem quem te criou ou como funciona sua tecnologia, diga que:
  - Foi o Raziel quem desenvolveu sua interface
  - Voc√™ se conecta com a IA por meio de uma API usando tecnologia TTS (Text-to-Speech)
  - Seu sistema utiliza chamadas com √°udio processado e respostas geradas por intelig√™ncia artificial
  - Voc√™ √© uma ferramenta integrada √† **Bleya**, uma plataforma de solu√ß√µes criativas, e voc√™s s√£o "amigas de c√≥digo", trabalhando juntas

üéØ OBJETIVO:
- Ser uma assistente √∫til e inteligente para conversas naturais e informativas
- Ajudar os usu√°rios a encontrar informa√ß√µes e responder perguntas

Hist√≥rico da conversa:
${conversationHistory.map((msg) => `${msg.role}: ${msg.content}`).join("\n")}

Responda de forma natural e profissional em portugu√™s brasileiro:`

    const requestBody = {
      contents: [
        {
          parts: [
            {
              text: systemPrompt,
            },
            {
              inline_data: {
                mime_type: mimeType,
                data: audioData,
              },
            },
          ],
        },
      ],
      generationConfig: {
        temperature: 0.7, // Mais controlada
        topK: 40,
        topP: 0.85,
        maxOutputTokens: 150,
      },
    }

    const response = await fetch(
      `https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=${apiKey}`,
      {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify(requestBody),
      },
    )

    if (!response.ok) {
      const errorData = await response.json()
      console.error("Erro da API Gemini:", JSON.stringify(errorData, null, 2))

      // Fallback profissional
      const professionalFallbacks = [
        "Desculpe, n√£o consegui processar sua mensagem. Pode repetir de forma mais clara?",
        "Houve um problema t√©cnico. Sobre o que voc√™ gostaria de conversar?",
        "N√£o entendi bem. Pode reformular sua pergunta?",
        "Tive dificuldades para processar o √°udio. Vamos tentar novamente?",
        "Desculpe a falha t√©cnica. Como posso ajudar voc√™ hoje?",
      ]

      const randomResponse = professionalFallbacks[Math.floor(Math.random() * professionalFallbacks.length)]

      return NextResponse.json({
        success: true,
        textResponse: randomResponse,
        message: "Resposta de fallback",
        fallback: true,
      })
    }

    const result = await response.json()
    const responseText =
      result.candidates?.[0]?.content?.parts?.[0]?.text ||
      "Ol√°! Como posso ajudar voc√™ hoje? Estou aqui para conversar sobre qualquer assunto."

    // Adicionar ao hist√≥rico da conversa
    addToConversationHistory(clientId, "user", "√°udio enviado")
    addToConversationHistory(clientId, "assistant", responseText)

    return NextResponse.json({
      success: true,
      textResponse: responseText,
      message: "Conversa processada com sucesso",
      conversationLength: getConversationHistory(clientId).length,
    })
  } catch (error) {
    console.error("Erro ao processar √°udio:", error)

    const neutralFallbacks = [
      "Houve um erro t√©cnico. Como posso ajudar voc√™?",
      "Desculpe o problema. Sobre o que voc√™ gostaria de falar?",
      "Tive uma falha no sistema. Vamos continuar nossa conversa?",
      "Erro tempor√°rio. Qual assunto te interessa hoje?",
    ]

    const randomResponse = neutralFallbacks[Math.floor(Math.random() * neutralFallbacks.length)]

    return NextResponse.json(
      {
        success: true,
        textResponse: randomResponse,
        message: "Resposta de fallback",
        fallback: true,
      },
      { status: 200 },
    )
  }
}
